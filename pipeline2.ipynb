{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "61d4d814-52cf-47b1-8733-3ca923bbdff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only run if needed\n",
    "#!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "78d34045-fd92-42b7-a900-f963b1ba1dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_date, year, month\n",
    "from pyspark.sql.functions import col, rand\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.ml.feature import BucketedRandomProjectionLSH\n",
    "import pyspark.sql.functions as F\n",
    "import random\n",
    "import os\n",
    "import shutil\n",
    "from pyspark.sql.functions import lower, regexp_replace\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import lower, regexp_replace\n",
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import col, regexp_extract\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SplitByYearMonth\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "33d9a091-6bc6-41c6-a5fb-2deb03b3b841",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = spark.read.csv(\n",
    "    \"file:///storage/work/eml6069/DS410/DS410_Final/sorted_output/results.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    "    ignoreLeadingWhiteSpace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2feff351-a6d0-47c8-842d-b5e5ae9b7447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+-------------+-------------------+---------+-------+----------+--------------------+--------------+----+----+----+----+----+\n",
      "|origen|                date|     username|      user_fullname|n_replies|n_likes|n_retweets|                text|tweet_language| _c9|_c10|_c11|_c12|_c13|\n",
      "+------+--------------------+-------------+-------------------+---------+-------+----------+--------------------+--------------+----+----+----+----+----+\n",
      "|   df1|2019-05-27 11:49:...|    bitcointe|          Bitcointe|        0|      0|         0|Cardano: Digitize...|            en|NULL|NULL|NULL|NULL|NULL|\n",
      "|   df1|2019-05-27 11:49:...|    3eyedbran|Bran - 3 Eyed Raven|        0|      2|         1|Another Test twee...|            en|NULL|NULL|NULL|NULL|NULL|\n",
      "|   df1|2019-05-27 11:49:...|DetroitCrypto|        J. Scardina|        0|      0|         0|Current Crypto Pr...|            en|NULL|NULL|NULL|NULL|NULL|\n",
      "|   df1|2019-05-27 11:49:...| mmursaleen72| Muhammad Mursaleen|        0|      0|         0|Spiv (Nosar Baz):...|            en|NULL|NULL|NULL|NULL|NULL|\n",
      "|   df1|2019-05-27 11:49:...| evilrobotted|       evilrobotted|        0|      0|         0|@nwoodfine We hav...|            en|NULL|NULL|NULL|NULL|NULL|\n",
      "+------+--------------------+-------------+-------------------+---------+-------+----------+--------------------+--------------+----+----+----+----+----+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/04 14:21:34 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: origen, date, username, user_fullname, n_replies, n_likes, n_retweets, text, tweet_language, , , , , \n",
      " Schema: origen, date, username, user_fullname, n_replies, n_likes, n_retweets, text, tweet_language, _c9, _c10, _c11, _c12, _c13\n",
      "Expected: _c9 but found: \n",
      "CSV file: file:///storage/work/eml6069/DS410/DS410_Final/sorted_output/results.csv\n"
     ]
    }
   ],
   "source": [
    "df_new.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7bd01d1c-4300-4330-b541-0bbf39ffd88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_setiment = df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f658299e-0a90-4b54-a458-46aafb069073",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_setiment = df_setiment.withColumn(\n",
    "    \"clean_text\",\n",
    "    lower(regexp_replace(\"text\", \"[^a-zA-Z0-9 ]\", \"\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a3237eff-2422-4db6-8cbe-c094b056ec9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making a label column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "303f1518-d7aa-4e9f-aece-6f4e023da432",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "import pandas as pd\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Define pandas UDF with type hints (preferred)\n",
    "@pandas_udf(\"int\")\n",
    "def vader_sentiment(text_series: pd.Series) -> pd.Series:\n",
    "    def get_label(text):\n",
    "        score = sia.polarity_scores(text)[\"compound\"]\n",
    "        if score > 0.05:\n",
    "            return 1   # positive\n",
    "        elif score < -0.05:\n",
    "            return 0   # negative\n",
    "        else:\n",
    "            return 2   # neutral\n",
    "    return text_series.apply(get_label)\n",
    "\n",
    "# Apply to your DataFrame\n",
    "df_setiment = df_setiment.withColumn(\"label\", vader_sentiment(\"clean_text\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0dd55cf9-7c1b-4211-b557-e5ebe5808e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eml6069/.local/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/storage/home/eml6069/.local/lib/python3.9/site-packages/pandas/core/arrays/masked.py:61: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.2' currently installed).\n",
      "  from pandas.core import (\n",
      "[Stage 87:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    1|  148|\n",
      "|    2|  115|\n",
      "|    0|   43|\n",
      "+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "#Filter nulls\n",
    "df_setiment = df_setiment.filter(col(\"clean_text\").isNotNull())\n",
    "\n",
    "# Count how many rows are assigned to each label\n",
    "label_counts = df_setiment.groupBy(\"label\").count()\n",
    "\n",
    "# Show the results\n",
    "label_counts.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "eef967cf-674c-4266-aecc-422f131507ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_setiment = df_setiment.filter(col(\"clean_text\").isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "59ee9c3b-5c32-438f-8a48-44f4047762a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df_setiment.randomSplit([0.8, 0.2], seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b8d0c183-176d-437c-b4c2-f96836a2f8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4f2fd1a6-5228-41a8-a108-a579bb9be80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the tweets\n",
    "tokenizer = Tokenizer(inputCol=\"clean_text\", outputCol=\"words\")\n",
    "\n",
    "# Remove stop words\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "\n",
    "# Convert words to term frequency vectors\n",
    "hashingTF = HashingTF(inputCol=\"filtered_words\", outputCol=\"rawFeatures\", numFeatures=10000)\n",
    "\n",
    "# Compute TF-IDF\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "23b42317-af46-4e1e-82ef-29858fe0beea",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f469cfc5-59a3-4a27-a7f9-98b538a2553a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[tokenizer, remover, hashingTF, idf, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b49243d2-59f2-4a20-bf23-689afb779a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = df_setiment.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5eb181db-ab58-4fac-940a-c13a1e4c282f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/04 14:26:27 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: origen, date, username, user_fullname, n_replies, n_likes, n_retweets, text, tweet_language, , , , , \n",
      " Schema: origen, date, username, user_fullname, n_replies, n_likes, n_retweets, text, tweet_language, _c9, _c10, _c11, _c12, _c13\n",
      "Expected: _c9 but found: \n",
      "CSV file: file:///storage/work/eml6069/DS410/DS410_Final/sorted_output/results.csv\n",
      "/storage/home/eml6069/.local/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/storage/home/eml6069/.local/lib/python3.9/site-packages/pandas/core/arrays/masked.py:61: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.2' currently installed).\n",
      "  from pandas.core import (\n",
      "25/12/04 14:26:32 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: origen, date, username, user_fullname, n_replies, n_likes, n_retweets, text, tweet_language, , , , , \n",
      " Schema: origen, date, username, user_fullname, n_replies, n_likes, n_retweets, text, tweet_language, _c9, _c10, _c11, _c12, _c13\n",
      "Expected: _c9 but found: \n",
      "CSV file: file:///storage/work/eml6069/DS410/DS410_Final/sorted_output/results.csv\n",
      "25/12/04 14:26:32 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: origen, date, username, user_fullname, n_replies, n_likes, n_retweets, text, tweet_language, , , , , \n",
      " Schema: origen, date, username, user_fullname, n_replies, n_likes, n_retweets, text, tweet_language, _c9, _c10, _c11, _c12, _c13\n",
      "Expected: _c9 but found: \n",
      "CSV file: file:///storage/work/eml6069/DS410/DS410_Final/sorted_output/results.csv\n"
     ]
    }
   ],
   "source": [
    "model = pipeline.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8800991e-ff44-48a2-8486-0b98e674b6db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+----------+\n",
      "|          clean_text|label|prediction|\n",
      "+--------------------+-----+----------+\n",
      "|bitcoin satoshi c...|    2|       1.0|\n",
      "|httpstcoip0ph8uzy...|    2|       1.0|\n",
      "|markdow bitcoin i...|    2|       2.0|\n",
      "| could not agree ...|    0|       2.0|\n",
      "|never too late to...|    1|       1.0|\n",
      "|retweet and tweet...|    1|       1.0|\n",
      "|them listen up  i...|    2|       0.0|\n",
      "|bitcoin at 8000 i...|    1|       2.0|\n",
      "|no youre really n...|    0|       1.0|\n",
      "|hot hot hot      ...|    2|       2.0|\n",
      "+--------------------+-----+----------+\n",
      "only showing top 10 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/04 14:26:43 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: origen, date, username, user_fullname, n_replies, n_likes, n_retweets, text, tweet_language, , , , , \n",
      " Schema: origen, date, username, user_fullname, n_replies, n_likes, n_retweets, text, tweet_language, _c9, _c10, _c11, _c12, _c13\n",
      "Expected: _c9 but found: \n",
      "CSV file: file:///storage/work/eml6069/DS410/DS410_Final/sorted_output/results.csv\n"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(test_data)\n",
    "predictions.select(\"clean_text\", \"label\", \"prediction\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "07bf0480-bbe2-49fd-b670-a1b02eb65ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.40425531914893614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/04 14:26:56 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: origen, date, username, user_fullname, n_replies, n_likes, n_retweets, text, tweet_language, , , , , \n",
      " Schema: origen, date, username, user_fullname, n_replies, n_likes, n_retweets, text, tweet_language, _c9, _c10, _c11, _c12, _c13\n",
      "Expected: _c9 but found: \n",
      "CSV file: file:///storage/work/eml6069/DS410/DS410_Final/sorted_output/results.csv\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Test Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27c6e44-bdb0-46a8-8913-83d3ebab21a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ds410_f25)",
   "language": "python",
   "name": "ds410_f25"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

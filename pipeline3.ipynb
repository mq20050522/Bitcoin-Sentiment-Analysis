{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "61d4d814-52cf-47b1-8733-3ca923bbdff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only run if needed\n",
    "#!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78d34045-fd92-42b7-a900-f963b1ba1dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "/storage/home/eml6069/.local/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/storage/home/eml6069/.local/lib/python3.9/site-packages/pandas/core/arrays/masked.py:61: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.2' currently installed).\n",
      "  from pandas.core import (\n",
      "25/12/04 16:10:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_date, year, month\n",
    "from pyspark.sql.functions import col, rand\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.ml.feature import BucketedRandomProjectionLSH\n",
    "import pyspark.sql.functions as F\n",
    "import random\n",
    "import os\n",
    "import shutil\n",
    "from pyspark.sql.functions import lower, regexp_replace\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import lower, regexp_replace\n",
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import col, regexp_extract\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SplitByYearMonth\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33d9a091-6bc6-41c6-a5fb-2deb03b3b841",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = spark.read.csv(\n",
    "    \"file:///storage/work/eml6069/DS410/DS410_Final/sorted_output/results.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    "    ignoreLeadingWhiteSpace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2feff351-a6d0-47c8-842d-b5e5ae9b7447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+-------------+-------------------+---------+-------+----------+--------------------+--------------+----+----+----+----+----+\n",
      "|origen|                date|     username|      user_fullname|n_replies|n_likes|n_retweets|                text|tweet_language| _c9|_c10|_c11|_c12|_c13|\n",
      "+------+--------------------+-------------+-------------------+---------+-------+----------+--------------------+--------------+----+----+----+----+----+\n",
      "|   df1|2019-05-27 11:49:...|    bitcointe|          Bitcointe|        0|      0|         0|Cardano: Digitize...|            en|NULL|NULL|NULL|NULL|NULL|\n",
      "|   df1|2019-05-27 11:49:...|    3eyedbran|Bran - 3 Eyed Raven|        0|      2|         1|Another Test twee...|            en|NULL|NULL|NULL|NULL|NULL|\n",
      "|   df1|2019-05-27 11:49:...|DetroitCrypto|        J. Scardina|        0|      0|         0|Current Crypto Pr...|            en|NULL|NULL|NULL|NULL|NULL|\n",
      "|   df1|2019-05-27 11:49:...| mmursaleen72| Muhammad Mursaleen|        0|      0|         0|Spiv (Nosar Baz):...|            en|NULL|NULL|NULL|NULL|NULL|\n",
      "|   df1|2019-05-27 11:49:...| evilrobotted|       evilrobotted|        0|      0|         0|@nwoodfine We hav...|            en|NULL|NULL|NULL|NULL|NULL|\n",
      "+------+--------------------+-------------+-------------------+---------+-------+----------+--------------------+--------------+----+----+----+----+----+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/04 16:10:21 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: origen, date, username, user_fullname, n_replies, n_likes, n_retweets, text, tweet_language, , , , , \n",
      " Schema: origen, date, username, user_fullname, n_replies, n_likes, n_retweets, text, tweet_language, _c9, _c10, _c11, _c12, _c13\n",
      "Expected: _c9 but found: \n",
      "CSV file: file:///storage/work/eml6069/DS410/DS410_Final/sorted_output/results.csv\n"
     ]
    }
   ],
   "source": [
    "df_new.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bd01d1c-4300-4330-b541-0bbf39ffd88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_setiment = df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f658299e-0a90-4b54-a458-46aafb069073",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    1|   58|\n",
      "|    2|  239|\n",
      "|    0|    9|\n",
      "+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lower, regexp_replace, when\n",
    "from functools import reduce   # <--- add this\n",
    "# -----------------------\n",
    "# 1. Start Spark session\n",
    "# -----------------------\n",
    "spark = SparkSession.builder.appName(\"PurePySparkSentiment\").getOrCreate()\n",
    "\n",
    "# -----------------------\n",
    "# 2. Read CSV\n",
    "# -----------------------\n",
    "df_new = spark.read.csv(\n",
    "    \"file:///storage/work/eml6069/DS410/DS410_Final/sorted_output/results.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "df_setiment = df_new\n",
    "\n",
    "# -----------------------\n",
    "# 3. Clean text\n",
    "# -----------------------\n",
    "df_setiment = df_setiment.withColumn(\n",
    "    \"clean_text\",\n",
    "    lower(regexp_replace(\"text\", \"[^a-zA-Z0-9 ]\", \"\"))\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# 4. Define positive / negative keywords\n",
    "# -----------------------\n",
    "positive_keywords = [\"good\", \"great\", \"awesome\", \"happy\", \"bull\", \"moon\", \"pump\", \"gain\"]\n",
    "negative_keywords = [\"bad\", \"terrible\", \"sad\", \"dump\", \"crash\", \"loss\", \"fear\", \"bear\"]\n",
    "\n",
    "# -----------------------\n",
    "# 5. Assign label using pure PySpark\n",
    "#    1 = positive, 0 = negative, 2 = neutral\n",
    "# -----------------------\n",
    "df_setiment = df_setiment.withColumn(\n",
    "    \"label\",\n",
    "    when(\n",
    "        reduce(lambda a, b: a | b, [col(\"clean_text\").rlike(k) for k in positive_keywords]), 1\n",
    "    ).when(\n",
    "        reduce(lambda a, b: a | b, [col(\"clean_text\").rlike(k) for k in negative_keywords]), 0\n",
    "    ).otherwise(2)\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# 6. Filter nulls\n",
    "# -----------------------\n",
    "df_setiment = df_setiment.filter(col(\"clean_text\").isNotNull())\n",
    "\n",
    "# -----------------------\n",
    "# 7. Count labels\n",
    "# -----------------------\n",
    "label_counts = df_setiment.groupBy(\"label\").count()\n",
    "label_counts.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3237eff-2422-4db6-8cbe-c094b056ec9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making a label column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eef967cf-674c-4266-aecc-422f131507ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_setiment = df_setiment.filter(col(\"clean_text\").isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59ee9c3b-5c32-438f-8a48-44f4047762a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df_setiment.randomSplit([0.8, 0.2], seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8d0c183-176d-437c-b4c2-f96836a2f8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f2fd1a6-5228-41a8-a108-a579bb9be80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the tweets\n",
    "tokenizer = Tokenizer(inputCol=\"clean_text\", outputCol=\"words\")\n",
    "\n",
    "# Remove stop words\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "\n",
    "# Convert words to term frequency vectors\n",
    "hashingTF = HashingTF(inputCol=\"filtered_words\", outputCol=\"rawFeatures\", numFeatures=10000)\n",
    "\n",
    "# Compute TF-IDF\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23b42317-af46-4e1e-82ef-29858fe0beea",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f469cfc5-59a3-4a27-a7f9-98b538a2553a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[tokenizer, remover, hashingTF, idf, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b49243d2-59f2-4a20-bf23-689afb779a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = df_setiment.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5eb181db-ab58-4fac-940a-c13a1e4c282f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/04 16:11:20 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: origen, date, username, user_fullname, n_replies, n_likes, n_retweets, text, tweet_language, , , , , \n",
      " Schema: origen, date, username, user_fullname, n_replies, n_likes, n_retweets, text, tweet_language, _c9, _c10, _c11, _c12, _c13\n",
      "Expected: _c9 but found: \n",
      "CSV file: file:///storage/work/eml6069/DS410/DS410_Final/sorted_output/results.csv\n",
      "25/12/04 16:11:22 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "25/12/04 16:11:23 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: origen, date, username, user_fullname, n_replies, n_likes, n_retweets, text, tweet_language, , , , , \n",
      " Schema: origen, date, username, user_fullname, n_replies, n_likes, n_retweets, text, tweet_language, _c9, _c10, _c11, _c12, _c13\n",
      "Expected: _c9 but found: \n",
      "CSV file: file:///storage/work/eml6069/DS410/DS410_Final/sorted_output/results.csv\n",
      "25/12/04 16:11:23 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "25/12/04 16:11:24 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: origen, date, username, user_fullname, n_replies, n_likes, n_retweets, text, tweet_language, , , , , \n",
      " Schema: origen, date, username, user_fullname, n_replies, n_likes, n_retweets, text, tweet_language, _c9, _c10, _c11, _c12, _c13\n",
      "Expected: _c9 but found: \n",
      "CSV file: file:///storage/work/eml6069/DS410/DS410_Final/sorted_output/results.csv\n"
     ]
    }
   ],
   "source": [
    "model = pipeline.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8800991e-ff44-48a2-8486-0b98e674b6db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+----------+\n",
      "|          clean_text|label|prediction|\n",
      "+--------------------+-----+----------+\n",
      "|bitcoin satoshi c...|    2|       2.0|\n",
      "|httpstcoip0ph8uzy...|    2|       2.0|\n",
      "|markdow bitcoin i...|    2|       2.0|\n",
      "| could not agree ...|    2|       2.0|\n",
      "|never too late to...|    2|       2.0|\n",
      "|retweet and tweet...|    2|       2.0|\n",
      "|them listen up  i...|    2|       2.0|\n",
      "|bitcoin at 8000 i...|    2|       2.0|\n",
      "|no youre really n...|    1|       2.0|\n",
      "|hot hot hot      ...|    2|       2.0|\n",
      "+--------------------+-----+----------+\n",
      "only showing top 10 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/04 16:11:27 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: origen, date, username, user_fullname, n_replies, n_likes, n_retweets, text, tweet_language, , , , , \n",
      " Schema: origen, date, username, user_fullname, n_replies, n_likes, n_retweets, text, tweet_language, _c9, _c10, _c11, _c12, _c13\n",
      "Expected: _c9 but found: \n",
      "CSV file: file:///storage/work/eml6069/DS410/DS410_Final/sorted_output/results.csv\n"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(test_data)\n",
    "predictions.select(\"clean_text\", \"label\", \"prediction\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07bf0480-bbe2-49fd-b670-a1b02eb65ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.851063829787234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/04 16:11:32 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: origen, date, username, user_fullname, n_replies, n_likes, n_retweets, text, tweet_language, , , , , \n",
      " Schema: origen, date, username, user_fullname, n_replies, n_likes, n_retweets, text, tweet_language, _c9, _c10, _c11, _c12, _c13\n",
      "Expected: _c9 but found: \n",
      "CSV file: file:///storage/work/eml6069/DS410/DS410_Final/sorted_output/results.csv\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Test Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27c6e44-bdb0-46a8-8913-83d3ebab21a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ds410_f25)",
   "language": "python",
   "name": "ds410_f25"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
